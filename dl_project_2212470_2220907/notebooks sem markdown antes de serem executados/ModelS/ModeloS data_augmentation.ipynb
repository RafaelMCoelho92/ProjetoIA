{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil\n",
    "train_dir = 'train'\n",
    "validation_dir = 'train3'\n",
    "test_dir = 'test'\n",
    "train_airplane_dir = 'train/000_airplane'\n",
    "train_automobile_dir = 'train/001_automobile'\n",
    "train_bird_dir = 'train/002_bird'\n",
    "train_cat_dir = 'train/003_cat'\n",
    "train_deer_dir = 'train/004_deer'\n",
    "train_dog_dir = 'train/005_dog'\n",
    "train_frog_dir = 'train/006_frog'\n",
    "train_horse_dir = 'train/007_horse'\n",
    "train_ship_dir = 'train/008_ship'\n",
    "train_truck_dir = 'train/009_truck'\n",
    "val_airplane_dir = 'train3/000_airplane'\n",
    "val_automobile_dir = 'train3/001_automobile'\n",
    "val_bird_dir = 'train3/002_bird'\n",
    "val_cat_dir = 'train3/003_cat'\n",
    "val_deer_dir = 'train3/004_deer'\n",
    "val_dog_dir = 'train3/005_dog'\n",
    "val_frog_dir = 'train3/006_frog'\n",
    "val_horse_dir = 'train3/007_horse'\n",
    "val_ship_dir = 'train3/008_ship'\n",
    "val_truck_dir = 'train3/009_truck'\n",
    "test_airplane_dir = 'test/test/000_airplane'\n",
    "test_automobile_dir = 'test/test/001_automobile'\n",
    "test_bird_dir = 'test/test/002_bird'\n",
    "test_cat_dir = 'test/test/003_cat'\n",
    "test_deer_dir = 'test/test/004_deer'\n",
    "test_dog_dir = 'test/test/005_dog'\n",
    "test_frog_dir = 'test/test/006_frog'\n",
    "test_horse_dir = 'test/test/007_horse'\n",
    "test_ship_dir = 'test/test/008_ship'\n",
    "test_truck_dir = 'test/test/009_truck'\n",
    "print('total training airplane images:', len(os.listdir(train_airplane_dir)))\n",
    "print('total training automobile images:', len(os.listdir(train_automobile_dir)))\n",
    "print('total training bird images:', len(os.listdir(train_bird_dir)))\n",
    "print('total training cat images:', len(os.listdir(train_cat_dir)))\n",
    "print('total training deer images:', len(os.listdir(train_deer_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dog_dir)))\n",
    "print('total training frog images:', len(os.listdir(train_frog_dir)))\n",
    "print('total training horse images:', len(os.listdir(train_horse_dir)))\n",
    "print('total training ship images:', len(os.listdir(train_ship_dir)))\n",
    "print('total training truck images:', len(os.listdir(train_truck_dir)))\n",
    "print('total validation airplane images:', len(os.listdir(val_airplane_dir)))\n",
    "print('total validation automobile images:', len(os.listdir(val_automobile_dir)))\n",
    "print('total validation bird images:', len(os.listdir(val_bird_dir)))\n",
    "print('total validation cat images:', len(os.listdir(val_cat_dir)))\n",
    "print('total validation deer images:', len(os.listdir(val_deer_dir)))\n",
    "print('total validation dog images:', len(os.listdir(val_dog_dir)))\n",
    "print('total validation frog images:', len(os.listdir(val_frog_dir)))\n",
    "print('total validation horse images:', len(os.listdir(val_horse_dir)))\n",
    "print('total validation ship images:', len(os.listdir(val_ship_dir)))\n",
    "print('total validation truck images:', len(os.listdir(val_truck_dir)))\n",
    "print('total testing airplane images:', len(os.listdir(test_airplane_dir)))\n",
    "print('total testing automobile images:', len(os.listdir(test_automobile_dir)))\n",
    "print('total testing bird images:', len(os.listdir(test_bird_dir)))\n",
    "print('total testing cat images:', len(os.listdir(test_cat_dir)))\n",
    "print('total testing deer images:', len(os.listdir(test_deer_dir)))\n",
    "print('total testing dog images:', len(os.listdir(test_dog_dir)))\n",
    "print('total testing frog images:', len(os.listdir(test_frog_dir)))\n",
    "print('total testing horse images:', len(os.listdir(test_horse_dir)))\n",
    "print('total testing ship images:', len(os.listdir(test_ship_dir)))\n",
    "print('total testing truck images:', len(os.listdir(test_truck_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import image_dataset_from_directory\n",
    "IMG_SIZE = 250\n",
    "train_dataset = image_dataset_from_directory(\n",
    "train_dir,\n",
    "image_size=(IMG_SIZE, IMG_SIZE),\n",
    "batch_size=32)\n",
    "validation_dataset = image_dataset_from_directory(\n",
    "validation_dir,\n",
    "image_size=(IMG_SIZE, IMG_SIZE),\n",
    "batch_size=32)\n",
    "test_dataset = image_dataset_from_directory(\n",
    "test_dir,\n",
    "image_size=(IMG_SIZE, IMG_SIZE),\n",
    "batch_size=32)\n",
    "import matplotlib.pyplot as plt\n",
    "for data_batch, _ in train_dataset.take(1):\n",
    "  for i in range(5):\n",
    "    plt.imshow(data_batch[i].numpy().astype(\"uint8\"))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "data_augmentation = keras.Sequential(\n",
    "  [\n",
    "  layers.RandomFlip(\"horizontal\"),\n",
    "  layers.RandomRotation(0.1),\n",
    "  layers.RandomZoom(0.2),\n",
    "  ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data_batch, labels_batch in train_dataset:\n",
    "  print('data batch shape:', data_batch.shape)\n",
    "  print('labels batch shape:', labels_batch.shape)\n",
    "  break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "from keras import models\n",
    "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
    "x = data_augmentation(inputs)\n",
    "x = layers.Rescaling(1./255)(inputs)\n",
    "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = layers.MaxPooling2D(pool_size=2)(x)\n",
    "x = layers.Flatten()(x)\n",
    "x = layers.Dense(512, activation=\"relu\")(x)\n",
    "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
    "model = keras.Model(inputs=inputs, outputs=outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "checkpoint_callback = ModelCheckpoint(filepath='models/CNN_modelS_with_data_augmentation.h5', \n",
    "                                      monitor='val_acc', \n",
    "                                      save_best_only=True,\n",
    "                                      save_weights_only=False,\n",
    "                                      verbose=1)\n",
    "\n",
    "early_stopping_callback = EarlyStopping(monitor='val_acc', \n",
    "                                        patience=5,\n",
    "                                        verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model.compile(\n",
    "  loss='sparse_categorical_crossentropy',\n",
    "  optimizer=tf.keras.optimizers.RMSprop(learning_rate=1e-4),\n",
    "  metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(\n",
    "  train_dataset,\n",
    "  epochs=50,\n",
    "  validation_data=validation_dataset,\n",
    "  callbacks=[checkpoint_callback, early_stopping_callback]\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "#model = keras.models.load_model('models/CNN_modelS_with_data_augmentation.h5')\n",
    "\n",
    "val_loss, val_acc = model.evaluate(validation_dataset)\n",
    "print('val_acc:', val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "epochs = range(1, len(acc) + 1)\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
