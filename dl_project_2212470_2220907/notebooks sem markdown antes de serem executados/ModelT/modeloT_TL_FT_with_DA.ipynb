{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBXwOAwx7AzB"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import image_dataset_from_directory\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from tensorflow import keras\n",
        "from keras import layers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_dirs = [\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train1',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train2',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train4',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train5'\n",
        "]\n",
        "validation_dir = '/content/drive/MyDrive/ProjetoIA/dataset/train3'\n",
        "test_dir = '/content/drive/MyDrive/ProjetoIA/dataset/test'\n",
        "\n",
        "\n",
        "IMG_SIZE = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "def load_and_concatenate_datasets(directories, img_size, batch_size):\n",
        "    datasets = [] \n",
        "    for directory in directories:\n",
        "        dataset = image_dataset_from_directory(\n",
        "            directory,\n",
        "            image_size=(img_size, img_size),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        datasets.append(dataset) \n",
        "    return datasets\n",
        "\n",
        "\n",
        "train_datasets = load_and_concatenate_datasets(train_dirs, IMG_SIZE, BATCH_SIZE)\n",
        "train_dataset = tf.data.Dataset.sample_from_datasets(train_datasets)\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "\n",
        "example_dataset = image_dataset_from_directory(\n",
        "    train_dirs[0],\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = example_dataset.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "conv_base = VGG16(weights=\"imagenet\", include_top=False)\n",
        "conv_base.trainable = True \n",
        "for layer in conv_base.layers[:-2]: \n",
        "    layer.trainable = False "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IKq9eBB97JyR"
      },
      "outputs": [],
      "source": [
        "data_augmentation = keras.Sequential(\n",
        "    [\n",
        "        layers.RandomFlip(\"horizontal\"),\n",
        "        layers.RandomRotation(0.1),\n",
        "        layers.RandomZoom(0.2),\n",
        "    ]\n",
        ")\n",
        "\n",
        "inputs = keras.Input(shape=(150, 150, 3)) \n",
        "x = data_augmentation(inputs) \n",
        "x = keras.applications.vgg16.preprocess_input(x)  \n",
        "x = conv_base(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(256, activation='relu')(x)\n",
        "x = layers.Dropout(0.5)(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dKwohYX47kHI"
      },
      "outputs": [],
      "source": [
        "model.compile(\n",
        "    loss=\"sparse_categorical_crossentropy\",\n",
        "    optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        "    metrics=[\"accuracy\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LL9yk66x7mmg"
      },
      "outputs": [],
      "source": [
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/ProjetoIA/models/CNN_modeloT_TL_FT_with_DA_best.h5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                           patience=5,\n",
        "                           verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "    train_dataset,\n",
        "    epochs=30,\n",
        "    validation_data=validation_dataset,\n",
        "    callbacks=[checkpoint_cb, early_stop]\n",
        ")\n",
        "\n",
        "model.save('/content/drive/MyDrive/ProjetoIA/models/CNN_modeloT_TL_FT_with_DA.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#We can later load it and test it:\n",
        "#loaded_model = keras.models.load_model('/content/drive/MyDrive/ProjetoIA/models/CNN_modeloT_TL_FT_with_DA.h5')\n",
        "val_loss, val_acc = model.evaluate(validation_dataset)\n",
        "print('val_acc:', val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
