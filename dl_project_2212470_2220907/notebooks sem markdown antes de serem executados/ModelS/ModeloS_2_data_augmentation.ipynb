{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.utils import image_dataset_from_directory\n",
        "\n",
        "train_dirs = [\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train1',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train2',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train4',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train5'\n",
        "]\n",
        "validation_dir = '/content/drive/MyDrive/ProjetoIA/dataset/train3'\n",
        "test_dir = '/content/drive/MyDrive/ProjetoIA/dataset/test'\n",
        "\n",
        "IMG_SIZE = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def load_and_concatenate_datasets(directories, img_size, batch_size):\n",
        "    datasets = []\n",
        "    for directory in directories:\n",
        "        dataset = image_dataset_from_directory(\n",
        "            directory,\n",
        "            image_size=(img_size, img_size),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "train_datasets = load_and_concatenate_datasets(train_dirs, IMG_SIZE, BATCH_SIZE)\n",
        "train_dataset = tf.data.Dataset.sample_from_datasets(train_datasets)\n",
        "\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "\n",
        "example_dataset = image_dataset_from_directory(\n",
        "    train_dirs[0],\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = example_dataset.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPP8YgGGhihO"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "data_augmentation = keras.Sequential(\n",
        "  [\n",
        "  layers.RandomFlip(\"horizontal\"),\n",
        "  layers.RandomRotation(0.1),\n",
        "  layers.RandomZoom(0.2),\n",
        "  ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "for data_batch, labels_batch in train_dataset:\n",
        "  print('data batch shape:', data_batch.shape)\n",
        "  print('labels batch shape:', labels_batch.shape)\n",
        "  break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ed4InANvhihO"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "from keras import models\n",
        "inputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n",
        "x = data_augmentation(inputs)\n",
        "x = layers.Rescaling(1./255)(inputs)\n",
        "x = layers.Conv2D(filters=32, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
        "x = layers.MaxPooling2D(pool_size=2)(x)\n",
        "x = layers.Flatten()(x)\n",
        "x = layers.Dense(512, activation=\"relu\")(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0mmJrK2ihihP"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "checkpoint_callback = ModelCheckpoint(filepath='/content/drive/MyDrive/ProjetoIA/models/modelS_2_data_augmentation_best.h5',\n",
        "                                      monitor='val_loss',\n",
        "                                      save_best_only=True,\n",
        "                                      save_weights_only=True,\n",
        "                                      verbose=1)\n",
        "\n",
        "early_stopping_callback = EarlyStopping(monitor='val_loss',\n",
        "                                        patience=6,\n",
        "                                        verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2w6qjwV5hihP"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "history = model.fit(\n",
        "  train_dataset,\n",
        "  epochs=50,\n",
        "  validation_data=validation_dataset,\n",
        "  callbacks=[checkpoint_callback, early_stopping_callback]\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "#model = keras.models.load_model('models/modelS_augmentation.h5')\n",
        "\n",
        "val_loss, val_acc = model.evaluate(validation_dataset)\n",
        "print('val_acc:', val_acc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
