{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from keras.utils import image_dataset_from_directory\n",
        "import tensorflow as tf\n",
        "\n",
        "train_dirs = [\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train1',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train2',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train4',\n",
        "    '/content/drive/MyDrive/ProjetoIA/dataset/train5'\n",
        "]\n",
        "validation_dir = '/content/drive/MyDrive/ProjetoIA/dataset/train3'\n",
        "test_dir = '/content/drive/MyDrive/ProjetoIA/dataset/test'\n",
        "\n",
        "\n",
        "IMG_SIZE = 150\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "\n",
        "def load_and_concatenate_datasets(directories, img_size, batch_size):\n",
        "    datasets = []\n",
        "    for directory in directories:\n",
        "        dataset = image_dataset_from_directory(\n",
        "            directory,\n",
        "            image_size=(img_size, img_size),\n",
        "            batch_size=batch_size\n",
        "        )\n",
        "        datasets.append(dataset)\n",
        "    return datasets\n",
        "\n",
        "\n",
        "train_datasets = load_and_concatenate_datasets(train_dirs, IMG_SIZE, BATCH_SIZE)\n",
        "train_dataset = tf.data.Dataset.sample_from_datasets(train_datasets)\n",
        "\n",
        "\n",
        "validation_dataset = image_dataset_from_directory(\n",
        "    validation_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "test_dataset = image_dataset_from_directory(\n",
        "    test_dir,\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "\n",
        "\n",
        "example_dataset = image_dataset_from_directory(\n",
        "    train_dirs[0],\n",
        "    image_size=(IMG_SIZE, IMG_SIZE),\n",
        "    batch_size=BATCH_SIZE\n",
        ")\n",
        "class_names = example_dataset.class_names\n",
        "print(class_names)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "conv_base = VGG16(weights='imagenet', include_top=False, input_shape=(150, 150, 3))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6n9pZkcaId7k"
      },
      "outputs": [],
      "source": [
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "def get_features_and_labels(dataset):\n",
        "  all_features = []\n",
        "  all_labels = []\n",
        "  for images, labels in dataset:\n",
        "    preprocessed_images = keras.applications.vgg16.preprocess_input(images)\n",
        "    features = conv_base.predict(preprocessed_images)\n",
        "    all_features.append(features)\n",
        "    all_labels.append(labels)\n",
        "  return np.concatenate(all_features), np.concatenate(all_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_features, train_labels = get_features_and_labels(train_dataset)\n",
        "val_features, val_labels = get_features_and_labels(validation_dataset)\n",
        "test_features, test_labels = get_features_and_labels(test_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xb0j5WGOIviJ"
      },
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "from keras import layers\n",
        "inputs = keras.Input(shape=(4, 4, 512))\n",
        "x = layers.Flatten()(inputs)\n",
        "x = layers.Dense(256)(x)\n",
        "x = layers.Dropout(0.2)(x)\n",
        "outputs = layers.Dense(10, activation=\"softmax\")(x)\n",
        "model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0D-DCw1l_zDv"
      },
      "outputs": [],
      "source": [
        "\n",
        "checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(\n",
        "    '/content/drive/MyDrive/ProjetoIA/models/modeloT_TL_FE_without_DA_best.h5',\n",
        "    save_best_only=True,\n",
        "    monitor='val_loss',\n",
        "    mode='min',\n",
        "    verbose=1\n",
        ")\n",
        "early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_loss',\n",
        "                           patience=5,\n",
        "                           verbose=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "model.compile(\n",
        " loss='sparse_categorical_crossentropy',\n",
        " optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        " metrics=['accuracy'])\n",
        "history = model.fit(\n",
        "  train_features, train_labels,\n",
        "  epochs=25,\n",
        "  validation_data=(val_features, val_labels),\n",
        "  callbacks=[checkpoint_cb, early_stop])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "import matplotlib.pyplot as plt\n",
        "acc = history.history['accuracy']\n",
        "val_acc = history.history['val_accuracy']\n",
        "loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(acc) + 1)\n",
        "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
        "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.figure()\n",
        "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DymgjAXRJm-g"
      },
      "outputs": [],
      "source": [
        "\n",
        "from keras import models\n",
        "inputs = keras.Input(shape=(150, 150, 3))\n",
        "x = keras.applications.vgg16.preprocess_input(inputs)\n",
        "x = conv_base(x)\n",
        "outputs = model(x)\n",
        "full_model = keras.Model(inputs, outputs)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oVXsgaKDJruM"
      },
      "outputs": [],
      "source": [
        "full_model.compile(\n",
        " loss='sparse_categorical_crossentropy',\n",
        " optimizer=keras.optimizers.RMSprop(learning_rate=1e-5),\n",
        " metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kmgDvTBrJuM7"
      },
      "outputs": [],
      "source": [
        "\n",
        "full_model.save('/content/drive/MyDrive/ProjetoIA/models/CNN_TL_FE_without_data_augmentation.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "from tensorflow import keras\n",
        "loaded_model = keras.models.load_model('/content/drive/MyDrive/ProjetoIA/models/modeloT_TL_FE_without_DA_best.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "val_loss, val_acc = full_model.evaluate(validation_dataset)\n",
        "print('val_acc:', val_acc)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
